Case study: data structure selection 案例学习：数据结构选择
===========================================================

At this point you have learned about Python’s core data structures, and
you have seen some of the algorithms that use them. If you would like to
know more about algorithms, this might be a good time to read
Chapter [algorithms]. But you don’t have to read it before you go on;
you can read it whenever you are interested.

目前为止，你已经学习了 Python 的核心数据结构，同时你也学会了使用它们的一些算法。如果希望学习更多的算法，
那么现在真是学习 [algorithms] 章的好时候。但是不必急着继续，你可以在你感兴趣的时候开始这个项目。

This chapter presents a case study with exercises that let you think
about choosing data structures and practice using them.

这一章主要通过案例学习启发你思考如何选择数据结构和在实践中使用它们。

Word frequency analysis 词频分析
--------------------------------

As usual, you should at least attempt the exercises before you read my
solutions.

和往常一样，在你阅读我的答案之前，你至少应该尝试解决一下下面的习题。

Write a program that reads a file, breaks each line into words, strips
whitespace and punctuation from the words, and converts them to
lowercase.

写一个程序，读取一个文件，每一行是一个单词，
删掉单词中的空格和标点，并且将它们转化为小写字母。

Hint: The string module provides a string named whitespace, which
contains space, tab, newline, etc., and punctuation which contains the
punctuation characters. Let’s see if we can make Python swear:

提示：string模块提供了名为whitespace的字符串，
其包括空格、制表符、新行等等，以及punctuation，
其包括标点字符。让我们看一下是否我们能让Python说脏话：

::

    >>> import string
    >>> string.punctuation
    '!"#$%&'()*+,-./:;<=>?@[\]^_`{|}~'

Also, you might consider using the string methods strip, replace and
translate.

同时，你可能需要考虑使用字符串方法strip、replace和translate。

Go to Project Gutenberg (http://gutenberg.org) and download your
favorite out-of-copyright book in plain text format.

前往古腾堡项目（\ `gutenberg.org <gutenberg.org>`__\ ）以纯文本格式下载你喜欢的版权已过期的图书。

Modify your program from the previous exercise to read the book you
downloaded, skip over the header information at the beginning of the
file, and process the rest of the words as before.

修改你前面习题的程序来读取你下载的书，
跳过文件开始的头部信息，并如前面一样处理其余的单词。

Then modify the program to count the total number of words in the book,
and the number of times each word is used.

然后修改程序来计算书中单词的总数， 以及每个单词被使用的次数。

Print the number of different words used in the book. Compare different
books by different authors, written in different eras. Which author uses
the most extensive vocabulary?

打印该书使用的不同单词的数目。 比较不同年代，不同作者写的不同的书。
哪个作者使用的词汇最大？

Modify the program from the previous exercise to print the 20 most
frequently used words in the book.

修改前面习题的程序来打印该书中最常使用的20个单词。

Modify the previous program to read a word list (see Section [wordlist])
and then print all the words in the book that are not in the word list.
How many of them are typos? How many of them are common words that
*should* be in the word list, and how many of them are really obscure?

修改前面的程序来读取一个词表（见[wordlist]节），
然后打印书中所有没有出现在该词表中的单词。 它们中有多少是拼写错误的？
有多少是词表中应该包括的常用词？ 有多少是生僻词？

Random numbers 随机数
---------------------

Given the same inputs, most computer programs generate the same outputs
every time, so they are said to be **deterministic**. Determinism is
usually a good thing, since we expect the same calculation to yield the
same result. For some applications, though, we want the computer to be
unpredictable. Games are an obvious example, but there are more.

给定相同的输入，大多数计算机程序每次生成相同的输出，
所以它们被称作\ **确定性的（deterministic）**\ 。
确定性通常是个好东西，因为我们期望相同的计算产生相同的结果。
然而，对于有些应用，我们期望计算机不可预知。
有戏是一个明显的例子，但是有更多的例子。

Making a program truly nondeterministic turns out to be difficult, but
there are ways to make it at least seem nondeterministic. One of them is
to use algorithms that generate **pseudorandom** numbers. Pseudorandom
numbers are not truly random because they are generated by a
deterministic computation, but just by looking at the numbers it is all
but impossible to distinguish them from random.

让一个程序真的非确定性并不容易，但是有办法使它看起来是非确定的。
其中之一是使用生成\ **伪随机（pseudorandom）**\ 数算法。
伪随机数不是真正的随机，因为它们由一个确定性的计算生成，
但是仅看其生成的数字，不可能将它们和随机生成的相区分开。

The random module provides functions that generate pseudorandom numbers
(which I will simply call “random” from here on).

random模块提供生成伪随机数的函数（从此之后，我将简单的称其为“随机”）。

The function random returns a random float between 0.0 and 1.0
(including 0.0 but not 1.0). Each time you call random, you get the next
number in a long series. To see a sample, run this loop:

函数random返回一个0.0到1.0之间的随机浮点数（包括0.0，但是不包括1.0）。
每次你调用random，你获得一个长序列中的下一个树。
为了看一个例子，运行此循环：

::

    import random

    for i in range(10):
        x = random.random()
        print(x)

The function randint takes parameters low and high and returns an
integer between low and high (including both).

函数randint接受参数low和high，
并返回一个low和high之间的整数（两个都包括）。

::

    >>> random.randint(5, 10)
    5
    >>> random.randint(5, 10)
    9

To choose an element from a sequence at random, you can use choice:

为了从一个序列中随机的选择一个元素，你可以使用choice：

::

    >>> t = [1, 2, 3]
    >>> random.choice(t)
    2
    >>> random.choice(t)
    3

The random module also provides functions to generate random values from
continuous distributions including Gaussian, exponential, gamma, and a
few more.

random模块也提供从包括高斯、指数、伽马以及更多连续分布中生成随机值的函数。

Write a function named ``choose_from_hist`` that takes a histogram as
defined in Section [histogram] and returns a random value from the
histogram, chosen with probability in proportion to frequency. For
example, for this histogram:

写一个名为\ ``choose_from_hist``\ 的函数，
其接受一个如[histogram]节定义的直方图，
并从该直方图中返回一个随机值，其选择概率和频率成正比。
例如，对于此直方图：

::

    >>> t = ['a', 'a', 'b']
    >>> hist = histogram(t)
    >>> hist
    {'a': 2, 'b': 1}

your function should return ``'a'`` with probability :math:`2/3` and
``'b'`` with probability :math:`1/3`.

你的函数应该返回’a’和概率\ :math:`2/3`\ 以及’b’和概率\ :math:`1/3`\ 。

Word histogram 单词直方图
-------------------------

You should attempt the previous exercises before you go on. You can
download my solution from http://thinkpython2.com/code/analyze_book1.py.
You will also need http://thinkpython2.com/code/emma.txt.

在你继续之前，你应该尝试前面的习题。
你可以从\ http://thinkpython2.com/code/analyze_book1.py\ 下载我的答案。
你也需要\ http://thinkpython2.com/code/emma.txt\ 。

Here is a program that reads a file and builds a histogram of the words
in the file:

这是一个读取一个文件并建立文件中单词直方图的程序。

::

    import string

    def process_file(filename):
        hist = dict()
        fp = open(filename)
        for line in fp:
            process_line(line, hist)
        return hist

    def process_line(line, hist):
        line = line.replace('-', ' ')
        
        for word in line.split():
            word = word.strip(string.punctuation + string.whitespace)
            word = word.lower()
            hist[word] = hist.get(word, 0) + 1

    hist = process_file('emma.txt')

This program reads emma.txt, which contains the text of *Emma* by Jane
Austen.

改程序读取emma.txt，其包括Jane Austen写的\ *Emma*\ 的文本。

``process_file`` loops through the lines of the file, passing them one
at a time to ``process_line``. The histogram hist is being used as an
accumulator.

``process_file``\ 循环读取每行文件，每次把它们传递给\ ``process_line``\ 。
直方图hist被用作一个累加器。

``process_line`` uses the string method replace to replace hyphens with
spaces before using split to break the line into a list of strings. It
traverses the list of words and uses strip and lower to remove
punctuation and convert to lower case. (It is a shorthand to say that
strings are “converted”; remember that strings are immutable, so methods
like strip and lower return new strings.)

在使用split将一行文件分成一个字符串列表之前，
``process_line``\ 使用字符串replace方法来用空格替换连字符。
它遍历单词的列表，并使用strip和lower来删除标点以及转化为小写。
（简称字符串被“转化”；记住字符串是不可变的，
所以类似strip和lower的方法返回新的字符串。）

Finally, ``process_line`` updates the histogram by creating a new item
or incrementing an existing one.

最后，\ ``process_line``\ 通过生成一个新的项或者递增一个已有的项来更新直方图。

To count the total number of words in the file, we can add up the
frequencies in the histogram:

为了计算文件中单词的总数，我们可以累加直方图中的频率：

::

    def total_words(hist):
        return sum(hist.values())

The number of different words is just the number of items in the
dictionary:

不同单词的数量恰好是词典中项的数目：

::

    def different_words(hist):
        return len(hist)

Here is some code to print the results:

这是打印结果的代码：

::

    print('Total number of words:', total_words(hist))
    print('Number of different words:', different_words(hist))

And the results:

结果是：

::

    Total number of words: 161080
    Number of different words: 7214

Most common words 最常用单词
----------------------------

To find the most common words, we can make a list of tuples, where each
tuple contains a word and its frequency, and sort it.

为了找到最常用的单词，我们可以使用元组列表，每个元组包含单词和它的频率，然后排序这个列表。

The following function takes a histogram and returns a list of
word-frequency tuples:

下面的函数接受一个直方图并且返回一个
单词-频率的元组列表：

::

    def most_common(hist):
        t = []
        for key, value in hist.items():
            t.append((value, key))

        t.sort(reverse=True)
        return t

In each tuple, the frequency appears first, so the resulting list is
sorted by frequency. Here is a loop that prints the ten most common
words:

每一个元组中，频率在前，所以这个列表是按照频率排序。下面是输出最常用的十个单词的循环:

::

    t = most_common(hist)
    print('The most common words are:')
    for freq, word in t[:10]:
        print(word, freq, sep='\t')

I use the keyword argument sep to tell print to use a tab character as a
“separator”, rather than a space, so the second column is lined up. Here
are the results from *Emma*:

这儿我使用了tab键作为关键词sep的参数，这将把tab键作为分隔符而不是空格键，所以第二行将对齐。下面是小说*Emma*的结果:

::

    The most common words are:
    to      5242
    the     5205
    and     4897
    of      4295
    i       3191
    a       3130
    it      2529
    her     2483
    was     2400
    she     2364

This code can be simplified using the key parameter of the sort
function. If you are curious, you can read about it at
https://wiki.python.org/moin/HowTo/Sorting.

当然这儿的代码也可以通过sort函数的不同关键词参数进行简化。如果你感兴趣，可以阅读https://wiki.python.org/moin/HowTo/Sorting。

Optional parameters 可选形参
----------------------------

We have seen built-in functions and methods that take optional
arguments. It is possible to write programmer-defined functions with
optional arguments, too. For example, here is a function that prints the
most common words in a histogram

我们已经见过接受可变数目实参的函数和方法了。
写出具有可选实参的用户自定义函数也是有可能的。
例如，这是一个打印直方图中最常见单词的函数。

::

    def print_most_common(hist, num=10):
        t = most_common(hist)
        print('The most common words are:')
        for freq, word in t[:num]:
            print(word, freq, sep='\t')

The first parameter is required; the second is optional. The **default
value** of num is 10.

第一个形参是必须的；第二个是可选的。 num的\ **默认值（default
value）**\ 是10.

If you only provide one argument:

如果你只提供了一个参数:

::

    print_most_common(hist)

num gets the default value. If you provide two arguments:

num将使用默认值。如果你你提供两个参数:

::

    print_most_common(hist, 20)

num gets the value of the argument instead. In other words, the optional
argument **overrides** the default value.

num获得实参的值。换句话说，可选实参\ **覆盖（overrides）**\ 了默认值。

If a function has both required and optional parameters, all the
required parameters have to come first, followed by the optional ones.

如果一个函数同时有必选和可选两类形参，则所有的必选形参必须首先出现，
后面跟着可选的。

Dictionary subtraction 字典差集
-------------------------------

Finding the words from the book that are not in the word list from
words.txt is a problem you might recognize as set subtraction; that is,
we want to find all the words from one set (the words in the book) that
are not in the other (the words in the list).

从书中找到所有没出现在词表words.txt中的单词是一个差集问题。
也就是，我们想从一个集合中（书中的单词）找到所有没出现在另一个集合中
（列表中的单词）的单词。

subtract takes dictionaries d1 and d2 and returns a new dictionary that
contains all the keys from d1 that are not in d2. Since we don’t really
care about the values, we set them all to None.

subtract接受词典d1和d2，并返回一个新的词典，
其包括d1中的所有没出现在d2中的键。
既然我们并不真正关心值，我们将它们都设为None。

::

    def subtract(d1, d2):
        res = dict()
        for key in d1:
            if key not in d2:
                res[key] = None
        return res

To find the words in the book that are not in words.txt, we can use
``process_file`` to build a histogram for words.txt, and then subtract:

为了找到书中的没有出现在words.txt中的单词，
我们可以使用\ ``process_file``\ 来为words.txt构建一个直方图，
然后subtract：

::

    words = process_file('words.txt')
    diff = subtract(hist, words)

    print("Words in the book that aren't in the word list:")
    for word in diff.keys():
        print(word, end=' ')

Here are some of the results from *Emma*:

这是来自小说\ *Emma*\ 的一些结果：

::

    Words in the book that aren't in the word list:
    rencontre jane's blanche woodhouses disingenuousness 
    friend's venice apartment ...

Some of these words are names and possessives. Others, like “rencontre”,
are no longer in common use. But a few are common words that should
really be in the list!

这些单词一些是名字和所有歌。另外的，如“rencontre”不是常用单词。
但是有一些确实是真的应该包括在列表中的常用单词。

Python provides a data structure called set that provides many common
set operations. You can read about them in Section [sets], or read the
documentation at
http://docs.python.org/3/library/stdtypes.html#types-set.

Python　也提供了一个叫做集合的数据结构，集合提供了很多接合操作。你可以在 [sets] 章更仔细地阅读，或者在官网上阅读文档http://docs.python.org/3/library/stdtypes.html#types-set。

Write a program that uses set subtraction to find words in the book that
are not in the word list. Solution:
http://thinkpython2.com/code/analyze_book2.py.

写一个函数，使用集合的差集操作来找出一本书中不在work list中的单词。解答：http://thinkpython2.com/code/analyze_book2.py。

Random words 随机单词
---------------------

To choose a random word from the histogram, the simplest algorithm is to
build a list with multiple copies of each word, according to the
observed frequency, and then choose from the list:

为了从直方图中随机选择一个单词，最简单的算法是创建一个列表，
其中根据其出现的频率，每个单词都有多个拷贝， 然后从该列表中选择：

::

    def random_word(h):
        t = []
        for word, freq in h.items():
            t.extend([word] * freq)

        return random.choice(t)

The expression \* freq creates a list with freq copies of the string
word. The extend method is similar to append except that the argument is
a sequence.

表达式\* freq生成一个具有freq个字符串word拷贝的列表。
extend方法和append类似，除了其实参为一个序列外。

This algorithm works, but it is not very efficient; each time you choose
a random word, it rebuilds the list, which is as big as the original
book. An obvious improvement is to build the list once and then make
multiple selections, but the list is still big.

该算法好使，但是不是很有效；每次你选择一个随机单词，
它都重建列表，其和原来的书一样大。
一个明显的改进是创建列表一次，然后进行多次选择， 但是该列表仍然很大。

An alternative is:

一个替代是：

#. Use keys to get a list of the words in the book.

   使用keys来获得该书中单词的列表。

#. Build a list that contains the cumulative sum of the word frequencies
   (see Exercise [cumulative]). The last item in this list is the total
   number of words in the book, :math:`n`.

   创建一个包含单词频率累积和的列表（见习题[cumulative]）。
   此列表的最后一项是书中单词的数目\ :math:`n`\ 。

#. Choose a random number from 1 to :math:`n`. Use a bisection search
   (See Exercise [bisection]) to find the index where the random number
   would be inserted in the cumulative sum.

   选择一个从1到\ :math:`n`\ 的随机数。使用二分搜索（见习题[bisection]）
   找到该随机数应该被在累积和中插入的索引。

#. Use the index to find the corresponding word in the word list.

   使用该索引从单词列表中找到相应的单词。

[randhist]

Write a program that uses this algorithm to choose a random word from
the book. Solution: http://thinkpython2.com/code/analyze_book3.py.

写一个使用该算法从书中选择一个随机单词的程序。
答案：\ http://thinkpython2.com/code/analyze_book3.py\ 。

Markov analysis 马尔科夫分析
----------------------------

If you choose words from the book at random, you can get a sense of the
vocabulary, but you probably won’t get a sentence:

如果你从书中随机选择单词，那么你会感受到词表的概念，
你可能不会获得一个句子：

::

    this the small regard harriet which knightley's it most things

A series of random words seldom makes sense because there is no
relationship between successive words. For example, in a real sentence
you would expect an article like “the” to be followed by an adjective or
a noun, and probably not a verb or adverb.

一系列随机单词没有意义，因为相邻的单词没有关系。
例如，在一个真实的句子中，你可能期望“the”后面跟着一个形容词或者名称，
不可能是一个动词或者副词。

One way to measure these kinds of relationships is Markov analysis,
which characterizes, for a given sequence of words, the probability of
the words that might come next. For example, the song *Eric, the Half a
Bee* begins:

一种衡量这种关系的方法是马尔科夫分析，对于一个给定的单词序列，
其描述了接下来的单词的概率。 例如，歌曲\ *Eric, the Half a
Bee*\ 开始是：

    | Half a bee, philosophically,
    | Must, ipso facto, half not be.
    | But half the bee has got to be
    | Vis a vis, its entity. D’you see?
    | But can a bee be said to be
    | Or not to be an entire bee
    | When half the bee is not a bee
    | Due to some ancient injury?

In this text, the phrase “half the” is always followed by the word
“bee”, but the phrase “the bee” might be followed by either “has” or
“is”.

在此文本中，短语“half the”后面总是跟着单词“bee”， 但是短语“the
bee”则可能跟着“has”或者“is”。

The result of Markov analysis is a mapping from each prefix (like “half
the” and “the bee”) to all possible suffixes (like “has” and “is”).

马尔科夫分析的结果是从每个前缀（如“half the”和“the bee”）
映射到所有可能的后缀（如“has”和“is”）。

Given this mapping, you can generate a random text by starting with any
prefix and choosing at random from the possible suffixes. Next, you can
combine the end of the prefix and the new suffix to form the next
prefix, and repeat.

给定此映射，你可以以任意前缀开始并从可能的后缀中随机选择一个来生成一个随机文本。
接下来，你可以组合前缀的结尾和新的后缀形参下一个前缀，并重复下去。

For example, if you start with the prefix “Half a”, then the next word
has to be “bee”, because the prefix only appears once in the text. The
next prefix is “a bee”, so the next suffix might be “philosophically”,
“be” or “due”.

例如，如果你以前缀“Half a”开始，然后下一个但是必须是“bee”，
因为此前缀在文本中仅出现一次。下一个前缀是“a bee”，
所以下一个后缀可能是“philosophically”，“be”或“due”。

In this example the length of the prefix is always two, but you can do
Markov analysis with any prefix length.

此例中，前缀的长度总是2，但是你可以以任意前缀长度进行马尔科夫分析。
前缀的长度被称作此分析的“阶”。

Markov analysis:

马尔科夫分析：

#. Write a program to read a text from a file and perform Markov
   analysis. The result should be a dictionary that maps from prefixes
   to a collection of possible suffixes. The collection might be a list,
   tuple, or dictionary; it is up to you to make an appropriate choice.
   You can test your program with prefix length two, but you should
   write the program in a way that makes it easy to try other lengths.
   
   写一个程序，从一个文件中读取文本并执行马尔科夫分析。
   结果应该是一个字典，其从前缀映射到一个可能的后缀集合。
   此集合可以是一个列表、元组或字典；一切取决于你以做出合适的选择。
   你可以用长度为2的前缀进行测试，但是你应该让此程序很容易的支持其它长度。

#. Add a function to the previous program to generate random text based
   on the Markov analysis. Here is an example from *Emma* with prefix
   length 2:
   
   在前面的程序中加一个函数，基于马尔科夫分析生成随机文本。
   这是来自\ *Emma*\ 的前缀为2的一个例子：

       He was very clever, be it sweetness or be angry, ashamed or only
       amused, at such a stroke. She had never thought of Hannah till
       you were never meant for me?“ ”I cannot make speeches, Emma:" he
       soon cut it all himself.

   For this example, I left the punctuation attached to the words. The
   result is almost syntactically correct, but not quite. Semantically,
   it almost makes sense, but not quite.
   
   对于这个例子，我保留了附在词后面的标点符号。
   结果几乎是语法正确的，但不完全。 语义上讲，它几乎有意义，但也不完全。

   What happens if you increase the prefix length? Does the random text
   make more sense?
   
   如果你增加前缀的长度，会发生什么？ 随机文本更有意义是么？

#. Once your program is working, you might want to try a mash-up: if you
   combine text from two or more books, the random text you generate
   will blend the vocabulary and phrases from the sources in interesting
   ways.
   
   一旦你的程序工作，你可能想尝试一下混搭：
   如果你来自两本或更多书的文本，
   你生成的随机文本将以有趣的方式混合来自不同源的词表和短语。

Credit: This case study is based on an example from Kernighan and Pike,
*The Practice of Programming*, Addison-Wesley, 1999.

声明：此案例学习基于来自Kernighan and Pike, *The Practice of
Programming*, Addison-Wesley, 1999. 的一个示例。

You should attempt this exercise before you go on; then you can can
download my solution from http://thinkpython2.com/code/markov.py. You
will also need http://thinkpython2.com/code/emma.txt.

在你继续之前，你应该尝试此习题；
你可以从\ http://thinkpython2.com/code/markov.py \ 下载我的答案。
你也需要\ http://thinkpython2.com/code/emma.txt \ 。

Data structures 数据结构
------------------------

Using Markov analysis to generate random text is fun, but there is also
a point to this exercise: data structure selection. In your solution to
the previous exercises, you had to choose:

使用马尔科夫分析生成随机文本很有趣，
但是对此习题还有一点需要注意：数据结构选择。
在你的解决方案中，你不得不选择：

-  How to represent the prefixes.

   如何表示前缀。

-  How to represent the collection of possible suffixes.

   如何表示可能后缀的集合。

-  How to represent the mapping from each prefix to the collection of
   possible suffixes.

   如何表示从前缀到可能后缀集合的映射。

The last one is easy: a dictionary is the obvious choice for a mapping
from keys to corresponding values.

好的，最后一个很简单；我们曾见过的唯一的映射是字典，
所以它是很自然的选择。

For the prefixes, the most obvious options are string, list of strings,
or tuple of strings.

对于前缀，最明显的选项是字符串、字符串列表或者字符串元组。

For the suffixes, one option is a list; another is a histogram
(dictionary).

对于后缀，一个选择是列表；另一个是直方图（字典）。

How should you choose? The first step is to think about the operations
you will need to implement for each data structure. For the prefixes, we
need to be able to remove words from the beginning and add to the end.
For example, if the current prefix is “Half a”, and the next word is
“bee”, you need to be able to form the next prefix, “a bee”.

你如何选择呢？ 第一步是考虑对每个数据结构你需要实现的操作。
对于前缀，我们需要能从从开始删除单词并在最后加入单词。
例如，如果当前的前缀是“Half a”，下一个词是“bee”，
你需要能构成下一个前缀“a bee”。

Your first choice might be a list, since it is easy to add and remove
elements, but we also need to be able to use the prefixes as keys in a
dictionary, so that rules out lists. With tuples, you can’t append or
remove, but you can use the addition operator to form a new tuple:

你的第一个选择可能是列表，因为它能很容易的增加和删除元素，
但是我们也需要让前缀作为字典的键，因此淘汰了列表。
使用元组，你不能追加或删除，
但是你能使用额外的运算符来形成一个新的元组：

::

    def shift(prefix, word):
        return prefix[1:] + (word,)

shift takes a tuple of words, prefix, and a string, word, and forms a
new tuple that has all the words in prefix except the first, and word
added to the end.

shift接受一个单词元组prefix和一个字符串word，
并形成一个新的元组，其具有prefix中除第一个单词外的全部单词，
然后在结尾增加word。

For the collection of suffixes, the operations we need to perform
include adding a new suffix (or increasing the frequency of an existing
one), and choosing a random suffix.

对于后缀的集合，我们需要执行的运算包括增加一个新的后缀
（或者增加一个已有后缀的频度），并选择一个随机后缀。

Adding a new suffix is equally easy for the list implementation or the
histogram. Choosing a random element from a list is easy; choosing from
a histogram is harder to do efficiently (see Exercise [randhist]).

对于列表或者直方图，增加一个新的后缀一样容易。
从列表中选择一个随机元素很容易；
从一个直方图中有效的选择有一些难（见习题[randhist]）。

So far we have been talking mostly about ease of implementation, but
there are other factors to consider in choosing data structures. One is
run time. Sometimes there is a theoretical reason to expect one data
structure to be faster than other; for example, I mentioned that the in
operator is faster for dictionaries than for lists, at least when the
number of elements is large.

目前为止，我们主要讨论实现的难易，
但是选择数据结构还有其它的因素。一个是运行时间。
有时，一个数据结构比另一个快有理论原因；
例如，我提到过in运算符对于字典比对列表要快，
至少当元素的数目很大的时候。


But often you don’t know ahead of time which implementation will be
faster. One option is to implement both of them and see which is better.
This approach is called **benchmarking**. A practical alternative is to
choose the data structure that is easiest to implement, and then see if
it is fast enough for the intended application. If so, there is no need
to go on. If not, there are tools, like the profile module, that can
identify the places in a program that take the most time.

但是通常你事先不知道哪个实现更快。
一个选择是两个都实现，然后再看哪个更快。
此方法被称作\ **基准测试（benchmarking）**\ 。
一个实用的选择是选择最容易实现的数据结构，
然后看它对于拟定的应用是否足够快。 如果是的话，就不需要继续了。
如果不是，有一些工具，如profile模块， 其能识别一个程序中哪处最耗时。

The other factor to consider is storage space. For example, using a
histogram for the collection of suffixes might take less space because
you only have to store each word once, no matter how many times it
appears in the text. In some cases, saving space can also make your
program run faster, and in the extreme, your program might not run at
all if you run out of memory. But for many applications, space is a
secondary consideration after run time.

另外考虑的因素是存储空间。例如，对后缀集合使用直方图可能用更少的空间，
因为无论一个单词在文本中出现多少次，你只需要存储它一次。
在一些情况下，节省空间也能让你的程序更快，极端情况下，
如果内存溢出，你的程序可能根本不能运行。
但是对于许多应用，空间是运行时间之后的第二位考虑。

One final thought: in this discussion, I have implied that we should use
one data structure for both analysis and generation. But since these are
separate phases, it would also be possible to use one structure for
analysis and then convert to another structure for generation. This
would be a net win if the time saved during generation exceeded the time
spent in conversion.


最后一个思考：在此讨论中，我暗示对于分析和生成，
我们应该使用一种数据结构。但是既然这些事分离的步骤，
对于分析使用一种数据结构，然后对生成转到另一种结构也是可能的。
如果生成节省的时间超过了转化花费的时间，这将是一个极大的优势。

Debugging 调试
--------------

When you are debugging a program, and especially if you are working on a
hard bug, there are five things to try:

当你正在调试一个程序的时候，特别是如果你正在调试一个很难的错误，
有五件事需要试一下：

Reading:
    Examine your code, read it back to yourself, and check that it says
    what you meant to say.
    
    读：检查你的代码，你自己回头读它，并且检查它说的是否是你想说的。

Running:
    Experiment by making changes and running different versions. Often
    if you display the right thing at the right place in the program,
    the problem becomes obvious, but sometimes you have to build
    scaffolding.
    
    运行：通过修改和运行不同的版本来实验。
    通常，如果在程序中，你在正确的地方显示正确的东西，
    问题变得很明显，但是有时你不得不花些时间创建脚手架。

Ruminating:
    Take some time to think! What kind of error is it: syntax, runtime,
    or semantic? What information can you get from the error messages,
    or from the output of the program? What kind of error could cause
    the problem you’re seeing? What did you change last, before the
    problem appeared?
    
    沉思：花些时间思考！错误的类型是什么：语法、运行时、语义？
    你从错误信息或者程序的输出中能获得什么信息？
    什么类型的错误能引起你看到的问题？ 问题出现前，你最后的修改是什么？

Rubberducking:
    If you explain the problem to someone else, you sometimes find the
    answer before you finish asking the question. Often you don’t need
    the other person; you could just talk to a rubber duck. And that’s
    the origin of the well-known strategy called **rubber duck
    debugging**. I am not making this up; see
    https://en.wikipedia.org/wiki/Rubber_duck_debugging.
    
    小黄鸭调试法:如果你将你的问题解释给别人听，你有时会在你问答案之前解决问题。经常，你并不需要
    另外一个人;你可以对着一个小黄鸭说问题。那就是著名的小黄鸭调试法(**rubber duck
    debugging**)的由来。我不是在信口开河，你可以看看这个维基页面:https://en.wikipedia.org/wiki/Rubber_duck_debugging。
    
Retreating:
    At some point, the best thing to do is back off, undoing recent
    changes, until you get back to a program that works and that you
    understand. Then you can start rebuilding.
    
    回退：在某种情况下，最好的事情是回退，撤销最近的修改，
    直到你回到一个能工作并且你能理解的程序。 然后你可以开始重建。

Beginning programmers sometimes get stuck on one of these activities and
forget the others. Each activity comes with its own failure mode.

初级程序员有时陷入这些活动之一，并且忘记了其它的。
事实上，单纯割裂的使用这些方法都有失败的可能。

For example, reading your code might help if the problem is a
typographical error, but not if the problem is a conceptual
misunderstanding. If you don’t understand what your program does, you
can read it 100 times and never see the error, because the error is in
your head.

例如，如果程序是一个拍板错误，读代码可能有帮助，
但是如果问题是概念理解错误，则未必是这样。 如果你不理解你的程序做什么，
你可能读你的程序100遍，并从不会发现错误， 因为错误在你的头脑中。

Running experiments can help, especially if you run small, simple tests.
But if you run experiments without thinking or reading your code, you
might fall into a pattern I call “random walk programming”, which is the
process of making random changes until the program does the right thing.
Needless to say, random walk programming can take a long time.

运行实验可能会有帮助，特别是如果你运行小的、简单的测试。
但是，如果你不思考或者阅读你的代码而运行实验，
你可能陷入一种被我称作“随机游走编程”的模式中，
这是一个随机修改的过程，直到程序做了正确的事儿。
不用说，随机游走编程会花费很长的时间。

You have to take time to think. Debugging is like an experimental
science. You should have at least one hypothesis about what the problem
is. If there are two or more possibilities, try to think of a test that
would eliminate one of them.

你必须花时间思考。调试类似实验科学。
你应该至少有一个关于问题是什么的假设。
如果有两个或者更多的可能，试着考虑一个能消除其中一个的测试。

But even the best debugging techniques will fail if there are too many
errors, or if the code you are trying to fix is too big and complicated.
Sometimes the best option is to retreat, simplifying the program until
you get to something that works and that you understand.

但是，如果有太多的错误或则你正试图修复的代码太大、太复杂，
即使最好的调试技术也会失败。
有时，最好的选择是回退，简化程序，直到你获得一个好使的并且能理解的程序。

Beginning programmers are often reluctant to retreat because they can’t
stand to delete a line of code (even if it’s wrong). If it makes you
feel better, copy your program into another file before you start
stripping it down. Then you can copy the pieces back one at a time.

初级程序员经常不愿意回退，因为他们不愿意删除一行代码（即使它是错误的）。
如果这让你感觉好些，在你开始删除之前，将你的代码拷贝到另一个文件中。
然后你能一次性把它们粘贴回来。

Finding a hard bug requires reading, running, ruminating, and sometimes
retreating. If you get stuck on one of these activities, try the others.

找到一个很难的错误需要阅读、运行、沉思、和时而的回退。
如果你陷入其中的一个活动中，试一下其它的。

Glossary 术语表
---------------

deterministic:
    Pertaining to a program that does the same thing each time it runs,
    given the same inputs.
    
确定性的：
    如果有相同的输入，程序每次会执行相同的事情。

pseudorandom:
    Pertaining to a sequence of numbers that appears to be random, but
    is generated by a deterministic program.
    
伪随机:
    通过确定性的程序生成一系列看似随机的数字

default value:
    The value given to an optional parameter if no argument is provided.

默认值:
    赋给形参的值的，如果实参没有提供。

override:
    To replace a default value with an argument.
    
覆盖:
    用实参替代默认值

benchmarking:
    The process of choosing between data structures by implementing
    alternatives and testing them on a sample of the possible inputs.
    
基准测试:
    通过测试可能输入来在可能的数据结构实现中选择的过程。

rubber duck debugging:
    Debugging by explaining your problem to an inanimate object such as
    a rubber duck. Articulating the problem can help you solve it, even
    if the rubber duck doesn’t know Python.
    
小黄鸭调试法;
    通过像小黄鸭这样的非生物体解释你的问题来找出程序问题的排错方法。清晰地陈述问题，可以帮助你解决问题，
    即使小黄鸭并不了解 Python。

Exercises　习题
------------------

习题 13-1
^^^^^^^^^^

The “rank” of a word is its position in a list of words sorted by
frequency: the most common word has rank 1, the second most common has
rank 2, etc.

单词的"秩"是指单词在单词按照频率排序的列表中的位置:出现最多的单词，秩是1,第二多的单词，秩是2。

Zipf’s law describes a relationship between the ranks and frequencies of
words in natural languages (http://en.wikipedia.org/wiki/Zipf's_law).
Specifically, it predicts that the frequency, :math:`f`, of the word
with rank :math:`r` is:

Zipf定律(\ http://en.wikipedia.org/wiki/Zipf's_law \)描述了在自然语言中秩和单词出现频率的关系。特别是，它预测单词
出现的频率 :math:`f` 和它的秩 :math:`r` 符合如下关系:

.. math:: f = c r^{-s}

where :math:`s` and :math:`c` are parameters that depend on the
language and the text. If you take the logarithm of both sides of this
equation, you get:

这里　:math:`s`　和　:math:`c` 是依赖于语言和文本的参数。如果在上述等式两边取对数的话，你可以得到:

.. math:: \log f = \log c - s \log r

So if you plot log :math:`f` versus log :math:`r`, you should get a
straight line with slope :math:`-s` and intercept log :math:`c`.

所以如果你做 log :math:`f`　和　log :math:`r`　的图像，你可以得到一条以　:math:`-s`　为斜率和　:math:`c`　为截距的直线。

Write a program that reads a text from a file, counts word frequencies,
and prints one line for each word, in descending order of frequency,
with log :math:`f` and log :math:`r`. Use the graphing program of your
choice to plot the results and check whether they form a straight line.
Can you estimate the value of :math:`s`?

写一个程序来从文件读入文本，计算单词频率，倒序输出每个单词同时输出log :math:`f` 和 log :math:`r`的直线。
使用你选择的画图程序画出结果并检查是不是形成一条直线。你可以估算出 :math:`s` 的值吗？

Solution: http://thinkpython2.com/code/zipf.py. To run my solution, you
need the plotting module matplotlib. If you installed Anaconda, you
already have matplotlib; otherwise you might have to install it.

答案在：http://thinkpython2.com/code/zipf.py。如果你希望测试我的结果，你需要画图模块　matplotlib，当然如果你安装了
Anaconda，你已经有了matplotlib；否则你需要安装它。
